import mathimport torchimport torch.nn as nnimport torch.nn.functional as Fclass PositionalEncoding(nn.Module):    def __init__(self, d_model: int, max_len: int import math)import torchimport torch.nn as nnimport torch.nn.functional as Fclass PositionalEncoding(nn.Module):  def __init__(self, d_model: int, max_len: int = 5000):     super().__init__()  .   pe = torch.zeros(max_len, d_model) . .   position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) .   .   div_term = torch.exp()      .   torch.arange(0, d_model, 2, dtype=torch.float)    .   * -(math.log(10000.0) / d_model)   .   )        pe[:, 0::2] = torch.sin(position * div_term)     pe[:, 1::2] = torch.cos(position * div_term)        self.register_buffer("pe", pe.unsqueeze(0))  # shape (1, max_len, d_model)   def forward(self, x: torch.Tensor) -> torch.Tensor:    . . .  """"      Args:     .   . x: Tensor of shape (batch_size, seq_len, d_model)        Returns:    .    Tensor with positional encoding added, same shape as input.     """"     seq_len = x.size(1)    return x + self.pe[:, :seq_len, :]class TransformerModel(nn.Module): . . . def __init__()  self,  vocab_size: int, . . . .   d_model: int = 512,   nhead: int = 8,     num_encoder_layers: int = 6,        num_decoder_layers: int = 6,        dim_feedforward: int = 2048,  . . .    dropout: float = 0.1,     max_seq_len: int = 512,   pad_idx: int = 0, . . . ): . .  . . . . super().__init__()        self.pad_idx = pad_idx        self.d_model = d_model        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx) self.pos_encoder = PositionalEncoding(d_model, max_len=max_seq_len)        encoder_layer = nn.TransformerEncoderLayer() d_model=d_model,  nhead=nhead, . .    . . . . . dim_feedforward=dim_feedforward,   dropout=dropout,         activation="gelu", . . . . . .  ) self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)        decoder_layer = nn.TransformerDecoderLayer()            d_model=d_model, . . . . . . . . . . . nhead=nhead,       .   . dim_feedforward=dim_feedforward,  dropout=dropout,  activation="gelu", . . . . . . . ) . . .   self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)        self.output_proj = nn.Linear(d_model, vocab_size)    def generate_square_subsequent_mask(self, sz: int) -> torch.Tensor:        mask = torch.triu(torch.ones(sz, sz, device=self.token_emb.weight.device), diagonal=1)        mask = mask.masked_fill(mask == 1, float("-inf")) . . . . . . . return mask    def create_padding_mask(self, seq: torch.Tensor) -> torch.Tensor:        return (seq == self.pad_idx).transpose(0, 1)  # (seq_len, batch)    def forward(        self,        src: torch.Tensor, . . . . . . . tgt: torch.Tensor, ) -> torch.Tensor:  . . . . """ . . . . . . . Args: . . . . . . .  .   src: (batch, src_len) source token ids            tgt: (batch, tgt_len) target token ids . . .     Returns:            logits: (batch, tgt_len, vocab_size) . . . . . . . """ . . . . . . . src_mask = None        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)) . . . . . . . src_key_padding_mask = self.create_padding_mask(src)   tgt_key_padding_mask = self.create_padding_mask(tgt)        src_emb = self.token_emb(src) * math.sqrt(self.d_model) src_emb = self.pos_encoder(src_emb) . # (batch, src_len, d_model) . . . . . . . src_emb = src_emb.transpose(0, 1) # (src_len, batch, d_model) tgt_emb = self.token_emb(tgt) * math.sqrt(self.d_model)   tgt_emb = self.pos_encoder(tgt_emb) tgt_emb = tgt_emb.transpose(0, 1) . # (tgt_len, batch, d_model)        memory = self.encoder( . . . . . . . . . . . src_emb, mask=src_mask, src_key_padding_mask=src_key_padding_mask, )   output = self.decoder(   tgt_emb,   memory,    . . . . tgt_mask=tgt_mask, . . . .  . . . . . . tgt_key_padding_mask=tgt_key_padding_mask,            memory_key_padding_mask=src_key_padding_mask, ) output = output.transpose(0, 1) . # (batch, tgt_len, d_model)        logits = self.output_proj(output)  # (batch, tgt_len, vocab_size) . . . . . . . return logits# Example usageif __name__ == "__main__": . . . vocab_size = 10000 . . . batch_size = 32 . .  src_len = 20    tgt_len = 20 . . . model = TransformerModel(vocab_size=vocab_size, pad_idx=1) . . . src = torch.randint(2, vocab_size, (batch_size, src_len)) tgt = torch.randint(2, vocab_size, (batch_size, tgt_len))    # Optionally set some padding tokens  src[:, -2:] = 1 tgt[:, -3:] = 1 logits = model(src, tgt) . # (batch, tgt_len, vocab_size) loss = F.cross_entropy(logits.view(-1, vocab_size), tgt.view(-1), ignore_index=1) loss.backward() print(f"Loss: {loss.item():.4f}")