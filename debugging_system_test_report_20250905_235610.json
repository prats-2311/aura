{
  "test_execution_summary": {
    "timestamp": "2025-09-05T23:56:10.181842",
    "total_execution_time": 13.732280015945435,
    "total_tests": 19,
    "passed_tests": 4,
    "failed_tests": 15,
    "skipped_tests": 0,
    "overall_success_rate": 21.052631578947366
  },
  "test_suites": {
    "debugging_functionality": {
      "suite_name": "debugging_functionality",
      "total_tests": 4,
      "passed_tests": 0,
      "failed_tests": 4,
      "skipped_tests": 0,
      "execution_time": 2.0766851902008057,
      "test_results": [
        {
          "test_name": "TestDebuggingFunctionalityValidation::test_permission_validation_comprehensive -v",
          "status": "FAILED",
          "execution_time": 0.7435991764068604,
          "error_message": "ERROR: found no collectors for /Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py::TestDebuggingFunctionalityValidation::test_permission_validation_comprehensive\n\n",
          "details": {
            "stdout": "============================= test session starts ==============================\nplatform darwin -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /opt/anaconda3/envs/aura/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/prateeksrivastava/Documents/aura\nconfigfile: pytest.ini\nplugins: asyncio-1.1.0, anyio-4.10.0, mock-3.14.1\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 0 items / 1 error\n\n==================================== ERRORS ====================================\n_________ ERROR collecting tests/test_debugging_system_integration.py __________\nImportError while importing test module '/Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\n/opt/anaconda3/envs/aura/lib/python3.11/importlib/__init__.py:126: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntests/test_debugging_system_integration.py:26: in <module>\n    from modules.diagnostic_tools import DiagnosticTools\nE   ImportError: cannot import name 'DiagnosticTools' from 'modules.diagnostic_tools' (/Users/prateeksrivastava/Documents/aura/modules/diagnostic_tools.py)\n=========================== short test summary info ============================\nERROR tests/test_debugging_system_integration.py\n=============================== 1 error in 0.36s ===============================\n",
            "stderr": "ERROR: found no collectors for /Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py::TestDebuggingFunctionalityValidation::test_permission_validation_comprehensive\n\n"
          }
        },
        {
          "test_name": "TestDebuggingFunctionalityValidation::test_accessibility_tree_inspection_comprehensive -v",
          "status": "FAILED",
          "execution_time": 0.4386920928955078,
          "error_message": "ERROR: found no collectors for /Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py::TestDebuggingFunctionalityValidation::test_accessibility_tree_inspection_comprehensive\n\n",
          "details": {
            "stdout": "============================= test session starts ==============================\nplatform darwin -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /opt/anaconda3/envs/aura/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/prateeksrivastava/Documents/aura\nconfigfile: pytest.ini\nplugins: asyncio-1.1.0, anyio-4.10.0, mock-3.14.1\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 0 items / 1 error\n\n==================================== ERRORS ====================================\n_________ ERROR collecting tests/test_debugging_system_integration.py __________\nImportError while importing test module '/Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\n/opt/anaconda3/envs/aura/lib/python3.11/importlib/__init__.py:126: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntests/test_debugging_system_integration.py:26: in <module>\n    from modules.diagnostic_tools import DiagnosticTools\nE   ImportError: cannot import name 'DiagnosticTools' from 'modules.diagnostic_tools' (/Users/prateeksrivastava/Documents/aura/modules/diagnostic_tools.py)\n=========================== short test summary info ============================\nERROR tests/test_debugging_system_integration.py\n=============================== 1 error in 0.22s ===============================\n",
            "stderr": "ERROR: found no collectors for /Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py::TestDebuggingFunctionalityValidation::test_accessibility_tree_inspection_comprehensive\n\n"
          }
        },
        {
          "test_name": "TestDebuggingFunctionalityValidation::test_element_detection_failure_analysis -v",
          "status": "FAILED",
          "execution_time": 0.4515042304992676,
          "error_message": "ERROR: found no collectors for /Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py::TestDebuggingFunctionalityValidation::test_element_detection_failure_analysis\n\n",
          "details": {
            "stdout": "============================= test session starts ==============================\nplatform darwin -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /opt/anaconda3/envs/aura/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/prateeksrivastava/Documents/aura\nconfigfile: pytest.ini\nplugins: asyncio-1.1.0, anyio-4.10.0, mock-3.14.1\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 0 items / 1 error\n\n==================================== ERRORS ====================================\n_________ ERROR collecting tests/test_debugging_system_integration.py __________\nImportError while importing test module '/Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\n/opt/anaconda3/envs/aura/lib/python3.11/importlib/__init__.py:126: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntests/test_debugging_system_integration.py:26: in <module>\n    from modules.diagnostic_tools import DiagnosticTools\nE   ImportError: cannot import name 'DiagnosticTools' from 'modules.diagnostic_tools' (/Users/prateeksrivastava/Documents/aura/modules/diagnostic_tools.py)\n=========================== short test summary info ============================\nERROR tests/test_debugging_system_integration.py\n=============================== 1 error in 0.23s ===============================\n",
            "stderr": "ERROR: found no collectors for /Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py::TestDebuggingFunctionalityValidation::test_element_detection_failure_analysis\n\n"
          }
        },
        {
          "test_name": "TestDebuggingFunctionalityValidation::test_comprehensive_diagnostics_execution -v",
          "status": "FAILED",
          "execution_time": 0.44162893295288086,
          "error_message": "ERROR: found no collectors for /Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py::TestDebuggingFunctionalityValidation::test_comprehensive_diagnostics_execution\n\n",
          "details": {
            "stdout": "============================= test session starts ==============================\nplatform darwin -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /opt/anaconda3/envs/aura/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/prateeksrivastava/Documents/aura\nconfigfile: pytest.ini\nplugins: asyncio-1.1.0, anyio-4.10.0, mock-3.14.1\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 0 items / 1 error\n\n==================================== ERRORS ====================================\n_________ ERROR collecting tests/test_debugging_system_integration.py __________\nImportError while importing test module '/Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\n/opt/anaconda3/envs/aura/lib/python3.11/importlib/__init__.py:126: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntests/test_debugging_system_integration.py:26: in <module>\n    from modules.diagnostic_tools import DiagnosticTools\nE   ImportError: cannot import name 'DiagnosticTools' from 'modules.diagnostic_tools' (/Users/prateeksrivastava/Documents/aura/modules/diagnostic_tools.py)\n=========================== short test summary info ============================\nERROR tests/test_debugging_system_integration.py\n=============================== 1 error in 0.22s ===============================\n",
            "stderr": "ERROR: found no collectors for /Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py::TestDebuggingFunctionalityValidation::test_comprehensive_diagnostics_execution\n\n"
          }
        }
      ],
      "success_rate": 0.0
    },
    "debugging_performance": {
      "suite_name": "debugging_performance",
      "total_tests": 2,
      "passed_tests": 0,
      "failed_tests": 2,
      "skipped_tests": 0,
      "execution_time": 0.8910970687866211,
      "test_results": [
        {
          "test_name": "TestDebuggingPerformanceImpact::test_debugging_overhead_measurement -v",
          "status": "FAILED",
          "execution_time": 0.4483768939971924,
          "error_message": "ERROR: found no collectors for /Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py::TestDebuggingPerformanceImpact::test_debugging_overhead_measurement\n\n",
          "details": {
            "stdout": "============================= test session starts ==============================\nplatform darwin -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /opt/anaconda3/envs/aura/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/prateeksrivastava/Documents/aura\nconfigfile: pytest.ini\nplugins: asyncio-1.1.0, anyio-4.10.0, mock-3.14.1\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 0 items / 1 error\n\n==================================== ERRORS ====================================\n_________ ERROR collecting tests/test_debugging_system_integration.py __________\nImportError while importing test module '/Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\n/opt/anaconda3/envs/aura/lib/python3.11/importlib/__init__.py:126: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntests/test_debugging_system_integration.py:26: in <module>\n    from modules.diagnostic_tools import DiagnosticTools\nE   ImportError: cannot import name 'DiagnosticTools' from 'modules.diagnostic_tools' (/Users/prateeksrivastava/Documents/aura/modules/diagnostic_tools.py)\n=========================== short test summary info ============================\nERROR tests/test_debugging_system_integration.py\n=============================== 1 error in 0.23s ===============================\n",
            "stderr": "ERROR: found no collectors for /Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py::TestDebuggingPerformanceImpact::test_debugging_overhead_measurement\n\n"
          }
        },
        {
          "test_name": "TestDebuggingPerformanceImpact::test_debug_level_performance_scaling -v",
          "status": "FAILED",
          "execution_time": 0.4421420097351074,
          "error_message": "ERROR: found no collectors for /Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py::TestDebuggingPerformanceImpact::test_debug_level_performance_scaling\n\n",
          "details": {
            "stdout": "============================= test session starts ==============================\nplatform darwin -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /opt/anaconda3/envs/aura/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/prateeksrivastava/Documents/aura\nconfigfile: pytest.ini\nplugins: asyncio-1.1.0, anyio-4.10.0, mock-3.14.1\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 0 items / 1 error\n\n==================================== ERRORS ====================================\n_________ ERROR collecting tests/test_debugging_system_integration.py __________\nImportError while importing test module '/Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\n/opt/anaconda3/envs/aura/lib/python3.11/importlib/__init__.py:126: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntests/test_debugging_system_integration.py:26: in <module>\n    from modules.diagnostic_tools import DiagnosticTools\nE   ImportError: cannot import name 'DiagnosticTools' from 'modules.diagnostic_tools' (/Users/prateeksrivastava/Documents/aura/modules/diagnostic_tools.py)\n=========================== short test summary info ============================\nERROR tests/test_debugging_system_integration.py\n=============================== 1 error in 0.22s ===============================\n",
            "stderr": "ERROR: found no collectors for /Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py::TestDebuggingPerformanceImpact::test_debug_level_performance_scaling\n\n"
          }
        }
      ],
      "success_rate": 0.0
    },
    "debugging_integration": {
      "suite_name": "debugging_integration",
      "total_tests": 2,
      "passed_tests": 0,
      "failed_tests": 2,
      "skipped_tests": 0,
      "execution_time": 0.9254231452941895,
      "test_results": [
        {
          "test_name": "TestDebuggingIntegrationWithExistingSystem::test_orchestrator_debugging_integration -v",
          "status": "FAILED",
          "execution_time": 0.46601128578186035,
          "error_message": "ERROR: found no collectors for /Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py::TestDebuggingIntegrationWithExistingSystem::test_orchestrator_debugging_integration\n\n",
          "details": {
            "stdout": "============================= test session starts ==============================\nplatform darwin -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /opt/anaconda3/envs/aura/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/prateeksrivastava/Documents/aura\nconfigfile: pytest.ini\nplugins: asyncio-1.1.0, anyio-4.10.0, mock-3.14.1\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 0 items / 1 error\n\n==================================== ERRORS ====================================\n_________ ERROR collecting tests/test_debugging_system_integration.py __________\nImportError while importing test module '/Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\n/opt/anaconda3/envs/aura/lib/python3.11/importlib/__init__.py:126: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntests/test_debugging_system_integration.py:26: in <module>\n    from modules.diagnostic_tools import DiagnosticTools\nE   ImportError: cannot import name 'DiagnosticTools' from 'modules.diagnostic_tools' (/Users/prateeksrivastava/Documents/aura/modules/diagnostic_tools.py)\n=========================== short test summary info ============================\nERROR tests/test_debugging_system_integration.py\n=============================== 1 error in 0.23s ===============================\n",
            "stderr": "ERROR: found no collectors for /Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py::TestDebuggingIntegrationWithExistingSystem::test_orchestrator_debugging_integration\n\n"
          }
        },
        {
          "test_name": "TestDebuggingIntegrationWithExistingSystem::test_accessibility_module_debugging_integration -v",
          "status": "FAILED",
          "execution_time": 0.45892906188964844,
          "error_message": "ERROR: found no collectors for /Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py::TestDebuggingIntegrationWithExistingSystem::test_accessibility_module_debugging_integration\n\n",
          "details": {
            "stdout": "============================= test session starts ==============================\nplatform darwin -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /opt/anaconda3/envs/aura/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/prateeksrivastava/Documents/aura\nconfigfile: pytest.ini\nplugins: asyncio-1.1.0, anyio-4.10.0, mock-3.14.1\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 0 items / 1 error\n\n==================================== ERRORS ====================================\n_________ ERROR collecting tests/test_debugging_system_integration.py __________\nImportError while importing test module '/Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\n/opt/anaconda3/envs/aura/lib/python3.11/importlib/__init__.py:126: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntests/test_debugging_system_integration.py:26: in <module>\n    from modules.diagnostic_tools import DiagnosticTools\nE   ImportError: cannot import name 'DiagnosticTools' from 'modules.diagnostic_tools' (/Users/prateeksrivastava/Documents/aura/modules/diagnostic_tools.py)\n=========================== short test summary info ============================\nERROR tests/test_debugging_system_integration.py\n=============================== 1 error in 0.22s ===============================\n",
            "stderr": "ERROR: found no collectors for /Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py::TestDebuggingIntegrationWithExistingSystem::test_accessibility_module_debugging_integration\n\n"
          }
        }
      ],
      "success_rate": 0.0
    },
    "real_world_scenarios": {
      "suite_name": "real_world_scenarios",
      "total_tests": 2,
      "passed_tests": 0,
      "failed_tests": 2,
      "skipped_tests": 0,
      "execution_time": 0.934345006942749,
      "test_results": [
        {
          "test_name": "TestDebuggingRealWorldScenarios::test_browser_debugging_scenarios -v",
          "status": "FAILED",
          "execution_time": 0.4465048313140869,
          "error_message": "ERROR: found no collectors for /Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py::TestDebuggingRealWorldScenarios::test_browser_debugging_scenarios\n\n",
          "details": {
            "stdout": "============================= test session starts ==============================\nplatform darwin -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /opt/anaconda3/envs/aura/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/prateeksrivastava/Documents/aura\nconfigfile: pytest.ini\nplugins: asyncio-1.1.0, anyio-4.10.0, mock-3.14.1\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 0 items / 1 error\n\n==================================== ERRORS ====================================\n_________ ERROR collecting tests/test_debugging_system_integration.py __________\nImportError while importing test module '/Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\n/opt/anaconda3/envs/aura/lib/python3.11/importlib/__init__.py:126: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntests/test_debugging_system_integration.py:26: in <module>\n    from modules.diagnostic_tools import DiagnosticTools\nE   ImportError: cannot import name 'DiagnosticTools' from 'modules.diagnostic_tools' (/Users/prateeksrivastava/Documents/aura/modules/diagnostic_tools.py)\n=========================== short test summary info ============================\nERROR tests/test_debugging_system_integration.py\n=============================== 1 error in 0.22s ===============================\n",
            "stderr": "ERROR: found no collectors for /Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py::TestDebuggingRealWorldScenarios::test_browser_debugging_scenarios\n\n"
          }
        },
        {
          "test_name": "TestDebuggingRealWorldScenarios::test_native_app_debugging_scenarios -v",
          "status": "FAILED",
          "execution_time": 0.48733997344970703,
          "error_message": "ERROR: found no collectors for /Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py::TestDebuggingRealWorldScenarios::test_native_app_debugging_scenarios\n\n",
          "details": {
            "stdout": "============================= test session starts ==============================\nplatform darwin -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /opt/anaconda3/envs/aura/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/prateeksrivastava/Documents/aura\nconfigfile: pytest.ini\nplugins: asyncio-1.1.0, anyio-4.10.0, mock-3.14.1\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 0 items / 1 error\n\n==================================== ERRORS ====================================\n_________ ERROR collecting tests/test_debugging_system_integration.py __________\nImportError while importing test module '/Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\n/opt/anaconda3/envs/aura/lib/python3.11/importlib/__init__.py:126: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntests/test_debugging_system_integration.py:26: in <module>\n    from modules.diagnostic_tools import DiagnosticTools\nE   ImportError: cannot import name 'DiagnosticTools' from 'modules.diagnostic_tools' (/Users/prateeksrivastava/Documents/aura/modules/diagnostic_tools.py)\n=========================== short test summary info ============================\nERROR tests/test_debugging_system_integration.py\n=============================== 1 error in 0.24s ===============================\n",
            "stderr": "ERROR: found no collectors for /Users/prateeksrivastava/Documents/aura/tests/test_debugging_system_integration.py::TestDebuggingRealWorldScenarios::test_native_app_debugging_scenarios\n\n"
          }
        }
      ],
      "success_rate": 0.0
    },
    "existing_system_integration": {
      "suite_name": "existing_system_integration",
      "total_tests": 9,
      "passed_tests": 4,
      "failed_tests": 5,
      "skipped_tests": 0,
      "execution_time": 8.903956413269043,
      "test_results": [
        {
          "test_name": "-v",
          "status": "PASSED",
          "execution_time": 0.45958495140075684,
          "error_message": null,
          "details": {
            "stdout": "============================= test session starts ==============================\nplatform darwin -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /opt/anaconda3/envs/aura/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/prateeksrivastava/Documents/aura\nconfigfile: pytest.ini\nplugins: asyncio-1.1.0, anyio-4.10.0, mock-3.14.1\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 11 items\n\ntests/test_debugging_comprehensive_simple.py::TestDebuggingComponentsBasic::test_accessibility_debugger_mock_test PASSED [  9%]\ntests/test_debugging_comprehensive_simple.py::TestDebuggingComponentsBasic::test_diagnostic_tools_mock_test PASSED [ 18%]\ntests/test_debugging_comprehensive_simple.py::TestDebuggingComponentsBasic::test_error_recovery_mock_test PASSED [ 27%]\ntests/test_debugging_comprehensive_simple.py::TestDebuggingComponentsBasic::test_integration_workflow_mock PASSED [ 36%]\ntests/test_debugging_comprehensive_simple.py::TestDebuggingComponentsBasic::test_permission_validator_mock_test PASSED [ 45%]\ntests/test_debugging_comprehensive_simple.py::TestPerformanceAndOverhead::test_mock_memory_usage_simulation PASSED [ 54%]\ntests/test_debugging_comprehensive_simple.py::TestPerformanceAndOverhead::test_mock_performance_measurement PASSED [ 63%]\ntests/test_debugging_comprehensive_simple.py::TestApplicationSpecificStrategies::test_finder_detection_strategy_mock PASSED [ 72%]\ntests/test_debugging_comprehensive_simple.py::TestApplicationSpecificStrategies::test_safari_detection_strategy_mock PASSED [ 81%]\ntests/test_debugging_comprehensive_simple.py::TestRealWorldScenarios::test_file_navigation_scenario_mock PASSED [ 90%]\ntests/test_debugging_comprehensive_simple.py::TestRealWorldScenarios::test_google_search_scenario_mock PASSED [100%]\n\n============================== 11 passed in 0.24s ==============================\n"
          }
        },
        {
          "test_name": "-v",
          "status": "FAILED",
          "execution_time": 0.4652090072631836,
          "error_message": "",
          "details": {
            "stdout": "============================= test session starts ==============================\nplatform darwin -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /opt/anaconda3/envs/aura/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/prateeksrivastava/Documents/aura\nconfigfile: pytest.ini\nplugins: asyncio-1.1.0, anyio-4.10.0, mock-3.14.1\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 13 items\n\ntests/test_debugging_integration_comprehensive.py::TestDebuggingWorkflowIntegration::test_comprehensive_diagnostic_workflow PASSED [  7%]\ntests/test_debugging_integration_comprehensive.py::TestDebuggingWorkflowIntegration::test_debugging_workflow_with_permission_issues PASSED [ 15%]\ntests/test_debugging_integration_comprehensive.py::TestDebuggingWorkflowIntegration::test_debugging_workflow_with_tree_traversal_issues PASSED [ 23%]\ntests/test_debugging_integration_comprehensive.py::TestDebuggingWorkflowIntegration::test_end_to_end_debugging_workflow_success PASSED [ 30%]\ntests/test_debugging_integration_comprehensive.py::TestApplicationSpecificDetectionStrategies::test_finder_specific_detection_strategy PASSED [ 38%]\ntests/test_debugging_integration_comprehensive.py::TestApplicationSpecificDetectionStrategies::test_safari_specific_detection_strategy PASSED [ 46%]\ntests/test_debugging_integration_comprehensive.py::TestApplicationSpecificDetectionStrategies::test_terminal_specific_detection_strategy PASSED [ 53%]\ntests/test_debugging_integration_comprehensive.py::TestPerformanceAndOverheadTests::test_debugging_tool_performance_overhead FAILED [ 61%]\ntests/test_debugging_integration_comprehensive.py::TestPerformanceAndOverheadTests::test_diagnostic_tool_effectiveness_measurement PASSED [ 69%]\ntests/test_debugging_integration_comprehensive.py::TestPerformanceAndOverheadTests::test_memory_usage_monitoring PASSED [ 76%]\ntests/test_debugging_integration_comprehensive.py::TestRealWorldScenarios::test_finder_file_navigation_scenario PASSED [ 84%]\ntests/test_debugging_integration_comprehensive.py::TestRealWorldScenarios::test_safari_google_search_scenario PASSED [ 92%]\ntests/test_debugging_integration_comprehensive.py::TestRealWorldScenarios::test_system_preferences_accessibility_scenario PASSED [100%]\n\n=================================== FAILURES ===================================\n___ TestPerformanceAndOverheadTests.test_debugging_tool_performance_overhead ___\n\nself = <tests.test_debugging_integration_comprehensive.TestPerformanceAndOverheadTests testMethod=test_debugging_tool_performance_overhead>\n\n    def test_debugging_tool_performance_overhead(self):\n        \"\"\"Test performance overhead of debugging tools.\"\"\"\n        # Test tree dump performance with different debug levels\n        debug_levels = ['BASIC', 'DETAILED', 'VERBOSE']\n        performance_results = {}\n    \n        for debug_level in debug_levels:\n            debugger = AccessibilityDebugger({\n                'debug_level': debug_level,\n                'max_tree_depth': 3,\n                'performance_tracking': True\n            })\n    \n            with patch.object(debugger, '_get_focused_application_name', return_value='TestApp'), \\\n                 patch.object(debugger, '_get_application_element', return_value=Mock()), \\\n                 patch.object(debugger, '_get_application_pid', return_value=1234), \\\n                 patch.object(debugger, '_traverse_accessibility_tree') as mock_traverse:\n    \n                # Setup mock tree traversal with varying complexity\n                mock_elements = []\n                for i in range(20):  # 20 elements\n                    mock_elements.append({\n                        'role': f'AXElement{i}',\n                        'title': f'Element {i}',\n                        'depth': i % 3,\n                        'all_attributes': {f'attr_{j}': f'value_{j}' for j in range(5)}\n                    })\n    \n                mock_traverse.return_value = (mock_elements[0], mock_elements)\n    \n                # Measure performance\n                start_time = time.time()\n                tree_dump = debugger.dump_accessibility_tree('TestApp')\n                end_time = time.time()\n    \n                performance_results[debug_level] = {\n                    'duration': end_time - start_time,\n                    'generation_time_ms': tree_dump.generation_time_ms,\n                    'total_elements': tree_dump.total_elements\n                }\n    \n        # Verify performance characteristics\n        # BASIC should be fastest, VERBOSE should be slowest\n>       self.assertLess(\n            performance_results['BASIC']['duration'],\n            performance_results['DETAILED']['duration']\n        )\nE       AssertionError: 8.392333984375e-05 not less than 4.076957702636719e-05\n\ntests/test_debugging_integration_comprehensive.py:625: AssertionError\n------------------------------ Captured log call -------------------------------\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\n=========================== short test summary info ============================\nFAILED tests/test_debugging_integration_comprehensive.py::TestPerformanceAndOverheadTests::test_debugging_tool_performance_overhead\n========================= 1 failed, 12 passed in 0.25s =========================\n",
            "stderr": ""
          }
        },
        {
          "test_name": "-v",
          "status": "FAILED",
          "execution_time": 0.19560599327087402,
          "error_message": "",
          "details": {
            "stdout": "============================= test session starts ==============================\nplatform darwin -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /opt/anaconda3/envs/aura/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/prateeksrivastava/Documents/aura\nconfigfile: pytest.ini\nplugins: asyncio-1.1.0, anyio-4.10.0, mock-3.14.1\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 0 items\n\n============================ no tests ran in 0.01s =============================\n",
            "stderr": ""
          }
        },
        {
          "test_name": "-v",
          "status": "FAILED",
          "execution_time": 0.4537222385406494,
          "error_message": "",
          "details": {
            "stdout": "============================= test session starts ==============================\nplatform darwin -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /opt/anaconda3/envs/aura/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/prateeksrivastava/Documents/aura\nconfigfile: pytest.ini\nplugins: asyncio-1.1.0, anyio-4.10.0, mock-3.14.1\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 7 items\n\ntests/test_accessibility_debugging_integration.py::TestAccessibilityDebuggingIntegration::test_debugging_tools_initialization FAILED [ 14%]\ntests/test_accessibility_debugging_integration.py::TestAccessibilityDebuggingIntegration::test_permission_validation_integration FAILED [ 28%]\ntests/test_accessibility_debugging_integration.py::TestAccessibilityDebuggingIntegration::test_find_element_enhanced_with_debugging FAILED [ 42%]\ntests/test_accessibility_debugging_integration.py::TestAccessibilityDebuggingIntegration::test_comprehensive_logging_integration FAILED [ 57%]\ntests/test_accessibility_debugging_integration.py::TestAccessibilityDebuggingIntegration::test_permission_change_callback_integration FAILED [ 71%]\ntests/test_accessibility_debugging_integration.py::TestAccessibilityDebuggingIntegration::test_debugging_failure_graceful_degradation PASSED [ 85%]\ntests/test_accessibility_debugging_integration.py::TestAccessibilityDebuggingIntegration::test_debugging_analysis_error_handling PASSED [100%]\n\n=================================== FAILURES ===================================\n__ TestAccessibilityDebuggingIntegration.test_debugging_tools_initialization ___\nFixture \"mock_permission_validator\" called directly. Fixtures are not meant to be called directly,\nbut are created automatically when test functions request them as parameters.\nSee https://docs.pytest.org/en/stable/explanation/fixtures.html for more information about fixtures, and\nhttps://docs.pytest.org/en/stable/deprecations.html#calling-fixtures-directly\n_ TestAccessibilityDebuggingIntegration.test_permission_validation_integration _\nFixture \"mock_permission_validator\" called directly. Fixtures are not meant to be called directly,\nbut are created automatically when test functions request them as parameters.\nSee https://docs.pytest.org/en/stable/explanation/fixtures.html for more information about fixtures, and\nhttps://docs.pytest.org/en/stable/deprecations.html#calling-fixtures-directly\n------------------------------ Captured log call -------------------------------\nWARNING  modules.accessibility:accessibility.py:540 Insufficient accessibility permissions for optimal performance\nINFO     modules.accessibility.operations:accessibility.py:869 AccessibilityModule initialized successfully\n_ TestAccessibilityDebuggingIntegration.test_find_element_enhanced_with_debugging _\nFixture \"mock_accessibility_debugger\" called directly. Fixtures are not meant to be called directly,\nbut are created automatically when test functions request them as parameters.\nSee https://docs.pytest.org/en/stable/explanation/fixtures.html for more information about fixtures, and\nhttps://docs.pytest.org/en/stable/deprecations.html#calling-fixtures-directly\n------------------------------ Captured log call -------------------------------\nINFO     modules.accessibility.operations:accessibility.py:869 AccessibilityModule initialized successfully\n_ TestAccessibilityDebuggingIntegration.test_comprehensive_logging_integration _\nFixture \"mock_accessibility_debugger\" called directly. Fixtures are not meant to be called directly,\nbut are created automatically when test functions request them as parameters.\nSee https://docs.pytest.org/en/stable/explanation/fixtures.html for more information about fixtures, and\nhttps://docs.pytest.org/en/stable/deprecations.html#calling-fixtures-directly\n------------------------------ Captured log call -------------------------------\nINFO     modules.accessibility.operations:accessibility.py:869 AccessibilityModule initialized successfully\n_ TestAccessibilityDebuggingIntegration.test_permission_change_callback_integration _\nFixture \"mock_permission_validator\" called directly. Fixtures are not meant to be called directly,\nbut are created automatically when test functions request them as parameters.\nSee https://docs.pytest.org/en/stable/explanation/fixtures.html for more information about fixtures, and\nhttps://docs.pytest.org/en/stable/deprecations.html#calling-fixtures-directly\n------------------------------ Captured log call -------------------------------\nWARNING  modules.accessibility:accessibility.py:540 Insufficient accessibility permissions for optimal performance\nINFO     modules.accessibility.operations:accessibility.py:869 AccessibilityModule initialized successfully\n=========================== short test summary info ============================\nFAILED tests/test_accessibility_debugging_integration.py::TestAccessibilityDebuggingIntegration::test_debugging_tools_initialization\nFAILED tests/test_accessibility_debugging_integration.py::TestAccessibilityDebuggingIntegration::test_permission_validation_integration\nFAILED tests/test_accessibility_debugging_integration.py::TestAccessibilityDebuggingIntegration::test_find_element_enhanced_with_debugging\nFAILED tests/test_accessibility_debugging_integration.py::TestAccessibilityDebuggingIntegration::test_comprehensive_logging_integration\nFAILED tests/test_accessibility_debugging_integration.py::TestAccessibilityDebuggingIntegration::test_permission_change_callback_integration\n========================= 5 failed, 2 passed in 0.24s ==========================\n",
            "stderr": ""
          }
        },
        {
          "test_name": "-v",
          "status": "PASSED",
          "execution_time": 0.3686180114746094,
          "error_message": null,
          "details": {
            "stdout": "============================= test session starts ==============================\nplatform darwin -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /opt/anaconda3/envs/aura/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/prateeksrivastava/Documents/aura\nconfigfile: pytest.ini\nplugins: asyncio-1.1.0, anyio-4.10.0, mock-3.14.1\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 35 items\n\ntests/test_accessibility_debugger.py::TestAccessibilityTreeElement::test_tree_element_creation PASSED [  2%]\ntests/test_accessibility_debugger.py::TestAccessibilityTreeElement::test_tree_element_to_dict PASSED [  5%]\ntests/test_accessibility_debugger.py::TestAccessibilityTreeElement::test_get_searchable_text PASSED [  8%]\ntests/test_accessibility_debugger.py::TestAccessibilityTreeElement::test_get_searchable_text_empty PASSED [ 11%]\ntests/test_accessibility_debugger.py::TestAccessibilityTreeDump::test_find_elements_by_text_exact PASSED [ 14%]\ntests/test_accessibility_debugger.py::TestAccessibilityTreeDump::test_find_elements_by_text_fuzzy PASSED [ 17%]\ntests/test_accessibility_debugger.py::TestAccessibilityTreeDump::test_get_elements_by_role PASSED [ 20%]\ntests/test_accessibility_debugger.py::TestAccessibilityTreeDump::test_to_json PASSED [ 22%]\ntests/test_accessibility_debugger.py::TestAccessibilityTreeDump::test_get_summary PASSED [ 25%]\ntests/test_accessibility_debugger.py::TestAccessibilityDebugger::test_debugger_initialization PASSED [ 28%]\ntests/test_accessibility_debugger.py::TestAccessibilityDebugger::test_debugger_initialization_defaults PASSED [ 31%]\ntests/test_accessibility_debugger.py::TestAccessibilityDebugger::test_debugger_initialization_no_accessibility PASSED [ 34%]\ntests/test_accessibility_debugger.py::TestAccessibilityDebugger::test_get_focused_application_name PASSED [ 37%]\ntests/test_accessibility_debugger.py::TestAccessibilityDebugger::test_get_focused_application_name_failure PASSED [ 40%]\ntests/test_accessibility_debugger.py::TestAccessibilityDebugger::test_get_application_element PASSED [ 42%]\ntests/test_accessibility_debugger.py::TestAccessibilityDebugger::test_get_application_element_not_found PASSED [ 45%]\ntests/test_accessibility_debugger.py::TestAccessibilityDebugger::test_get_application_pid PASSED [ 48%]\ntests/test_accessibility_debugger.py::TestAccessibilityDebugger::test_get_application_pid_not_found PASSED [ 51%]\ntests/test_accessibility_debugger.py::TestAccessibilityDebugger::test_extract_element_info PASSED [ 54%]\ntests/test_accessibility_debugger.py::TestAccessibilityDebugger::test_extract_element_info_with_exceptions PASSED [ 57%]\ntests/test_accessibility_debugger.py::TestAccessibilityDebugger::test_has_searchable_content PASSED [ 60%]\ntests/test_accessibility_debugger.py::TestAccessibilityDebugger::test_find_exact_matches PASSED [ 62%]\ntests/test_accessibility_debugger.py::TestAccessibilityDebugger::test_find_partial_matches PASSED [ 65%]\ntests/test_accessibility_debugger.py::TestAccessibilityDebugger::test_find_role_based_matches PASSED [ 68%]\ntests/test_accessibility_debugger.py::TestAccessibilityDebugger::test_deduplicate_matches PASSED [ 71%]\ntests/test_accessibility_debugger.py::TestAccessibilityDebugger::test_generate_search_recommendations_no_matches PASSED [ 74%]\ntests/test_accessibility_debugger.py::TestAccessibilityDebugger::test_generate_search_recommendations_multiple_matches PASSED [ 77%]\ntests/test_accessibility_debugger.py::TestAccessibilityDebugger::test_cleanup_tree_cache PASSED [ 80%]\ntests/test_accessibility_debugger.py::TestAccessibilityDebugger::test_traverse_accessibility_tree_max_depth PASSED [ 82%]\ntests/test_accessibility_debugger.py::TestAccessibilityDebugger::test_traverse_accessibility_tree_with_children PASSED [ 85%]\ntests/test_accessibility_debugger.py::TestAccessibilityDebugger::test_traverse_accessibility_tree_exception_handling PASSED [ 88%]\ntests/test_accessibility_debugger.py::TestElementAnalysisResult::test_analysis_result_creation PASSED [ 91%]\ntests/test_accessibility_debugger.py::TestElementAnalysisResult::test_analysis_result_to_dict PASSED [ 94%]\ntests/test_accessibility_debugger.py::TestAccessibilityDebuggerIntegration::test_full_tree_dump_workflow PASSED [ 97%]\ntests/test_accessibility_debugger.py::TestAccessibilityDebuggerIntegration::test_element_analysis_workflow PASSED [100%]\n\n============================== 35 passed in 0.16s ==============================\n"
          }
        },
        {
          "test_name": "-v",
          "status": "FAILED",
          "execution_time": 0.6671433448791504,
          "error_message": "",
          "details": {
            "stdout": "============================= test session starts ==============================\nplatform darwin -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /opt/anaconda3/envs/aura/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/prateeksrivastava/Documents/aura\nconfigfile: pytest.ini\nplugins: asyncio-1.1.0, anyio-4.10.0, mock-3.14.1\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 22 items\n\ntests/test_performance_monitoring_unit.py::TestPerformanceMonitoring::test_performance_metrics_data_model PASSED [  4%]\ntests/test_performance_monitoring_unit.py::TestPerformanceMonitoring::test_performance_metrics_timeout_detection PASSED [  9%]\ntests/test_performance_monitoring_unit.py::TestPerformanceMonitoring::test_performance_metrics_error_handling PASSED [ 13%]\ntests/test_performance_monitoring_unit.py::TestPerformanceMonitoring::test_fast_path_performance_report PASSED [ 18%]\ntests/test_performance_monitoring_unit.py::TestPerformanceMonitoring::test_performance_monitoring_enabled_configuration PASSED [ 22%]\ntests/test_performance_monitoring_unit.py::TestPerformanceMonitoring::test_performance_timing_measurement FAILED [ 27%]\ntests/test_performance_monitoring_unit.py::TestPerformanceMonitoring::test_performance_warning_threshold FAILED [ 31%]\ntests/test_performance_monitoring_unit.py::TestPerformanceMonitoring::test_performance_statistics_tracking PASSED [ 36%]\ntests/test_performance_monitoring_unit.py::TestPerformanceMonitoring::test_operation_metrics_tracking FAILED [ 40%]\ntests/test_performance_monitoring_unit.py::TestPerformanceMonitoring::test_performance_history_size_limit FAILED [ 45%]\ntests/test_performance_monitoring_unit.py::TestCachingFunctionality::test_fuzzy_match_cache_functionality FAILED [ 50%]\ntests/test_performance_monitoring_unit.py::TestCachingFunctionality::test_target_extraction_cache_functionality PASSED [ 54%]\ntests/test_performance_monitoring_unit.py::TestCachingFunctionality::test_cache_ttl_expiration FAILED [ 59%]\ntests/test_performance_monitoring_unit.py::TestCachingFunctionality::test_cache_size_limit_enforcement FAILED [ 63%]\ntests/test_performance_monitoring_unit.py::TestCachingFunctionality::test_cache_cleanup_functionality FAILED [ 68%]\ntests/test_performance_monitoring_unit.py::TestCachingFunctionality::test_cache_hit_miss_statistics PASSED [ 72%]\ntests/test_performance_monitoring_unit.py::TestCachingFunctionality::test_cache_integration_with_fuzzy_matching FAILED [ 77%]\ntests/test_performance_monitoring_unit.py::TestTimeoutHandling::test_timeout_configuration_loading PASSED [ 81%]\ntests/test_performance_monitoring_unit.py::TestTimeoutHandling::test_fuzzy_matching_timeout_handling PASSED [ 86%]\ntests/test_performance_monitoring_unit.py::TestTimeoutHandling::test_attribute_checking_timeout_handling FAILED [ 90%]\ntests/test_performance_monitoring_unit.py::TestTimeoutHandling::test_timeout_warning_logging FAILED [ 95%]\ntests/test_performance_monitoring_unit.py::TestTimeoutHandling::test_graceful_timeout_degradation FAILED [100%]\n\n=================================== FAILURES ===================================\n________ TestPerformanceMonitoring.test_performance_timing_measurement _________\n\nself = <tests.test_performance_monitoring_unit.TestPerformanceMonitoring object at 0x1177fbcd0>\naccessibility_module = <modules.accessibility.AccessibilityModule object at 0x117b23990>\n\n    def test_performance_timing_measurement(self, accessibility_module):\n        \"\"\"Test that operations are timed correctly.\"\"\"\n        # Mock a timed operation\n>       with patch.object(accessibility_module, '_record_performance_metric') as mock_record:\n\ntests/test_performance_monitoring_unit.py:152: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/opt/anaconda3/envs/aura/lib/python3.11/unittest/mock.py:1446: in __enter__\n    original, local = self.get_original()\n                      ^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <unittest.mock._patch object at 0x11780ba50>\n\n    def get_original(self):\n        target = self.getter()\n        name = self.attribute\n    \n        original = DEFAULT\n        local = False\n    \n        try:\n            original = target.__dict__[name]\n        except (AttributeError, KeyError):\n            original = getattr(target, name, DEFAULT)\n        else:\n            local = True\n    \n        if name in _builtins and isinstance(target, ModuleType):\n            self.create = True\n    \n        if not self.create and original is DEFAULT:\n>           raise AttributeError(\n                \"%s does not have the attribute %r\" % (target, name)\n            )\nE           AttributeError: <modules.accessibility.AccessibilityModule object at 0x117b23990> does not have the attribute '_record_performance_metric'\n\n/opt/anaconda3/envs/aura/lib/python3.11/unittest/mock.py:1419: AttributeError\n------------------------------ Captured log setup ------------------------------\nWARNING  modules.accessibility:accessibility.py:540 Insufficient accessibility permissions for optimal performance\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility:accessibility.py:619 Failed to initialize debugging tools: 'dict' object has no attribute 'validate'\nWARNING  modules.accessibility:accessibility.py:503 Accessibility permissions not granted: Insufficient accessibility permissions: PARTIAL\nWARNING  modules.accessibility:accessibility.py:716 AccessibilityModule entering degraded mode: permission_denied\n_________ TestPerformanceMonitoring.test_performance_warning_threshold _________\n\nself = <tests.test_performance_monitoring_unit.TestPerformanceMonitoring object at 0x117800410>\naccessibility_module = <modules.accessibility.AccessibilityModule object at 0x117be6690>\n\n    def test_performance_warning_threshold(self, accessibility_module):\n        \"\"\"Test performance warning threshold detection.\"\"\"\n        # Test with operation that exceeds warning threshold\n        slow_duration_ms = accessibility_module.performance_warning_threshold_ms + 100\n    \n        with patch.object(accessibility_module.performance_logger, 'warning') as mock_warning:\n            # Simulate recording a slow operation\n>           accessibility_module._check_performance_warning(\"slow_operation\", slow_duration_ms)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE           AttributeError: 'AccessibilityModule' object has no attribute '_check_performance_warning'\n\ntests/test_performance_monitoring_unit.py:178: AttributeError\n------------------------------ Captured log setup ------------------------------\nWARNING  modules.accessibility:accessibility.py:540 Insufficient accessibility permissions for optimal performance\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility:accessibility.py:619 Failed to initialize debugging tools: 'dict' object has no attribute 'validate'\nWARNING  modules.accessibility:accessibility.py:503 Accessibility permissions not granted: Insufficient accessibility permissions: PARTIAL\nWARNING  modules.accessibility:accessibility.py:716 AccessibilityModule entering degraded mode: permission_denied\n__________ TestPerformanceMonitoring.test_operation_metrics_tracking ___________\n\nself = <tests.test_performance_monitoring_unit.TestPerformanceMonitoring object at 0x117801210>\naccessibility_module = <modules.accessibility.AccessibilityModule object at 0x117826610>\n\n    def test_operation_metrics_tracking(self, accessibility_module):\n        \"\"\"Test that operation-specific metrics are tracked.\"\"\"\n        assert hasattr(accessibility_module, 'operation_metrics')\n        assert isinstance(accessibility_module.operation_metrics, dict)\n    \n        expected_operations = [\n            'element_role_checks',\n            'attribute_examinations',\n            'fuzzy_matching_operations',\n            'target_extractions',\n            'cache_operations'\n        ]\n    \n        for operation in expected_operations:\n            assert operation in accessibility_module.operation_metrics\n            assert 'count' in accessibility_module.operation_metrics[operation]\n            assert 'total_time_ms' in accessibility_module.operation_metrics[operation]\n>           assert 'success_count' in accessibility_module.operation_metrics[operation]\nE           AssertionError: assert 'success_count' in {'count': 0, 'hit_count': 0, 'total_time_ms': 0.0}\n\ntests/test_performance_monitoring_unit.py:222: AssertionError\n------------------------------ Captured log setup ------------------------------\nWARNING  modules.accessibility:accessibility.py:540 Insufficient accessibility permissions for optimal performance\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility:accessibility.py:619 Failed to initialize debugging tools: 'dict' object has no attribute 'validate'\nWARNING  modules.accessibility:accessibility.py:503 Accessibility permissions not granted: Insufficient accessibility permissions: PARTIAL\nWARNING  modules.accessibility:accessibility.py:716 AccessibilityModule entering degraded mode: permission_denied\n________ TestPerformanceMonitoring.test_performance_history_size_limit _________\n\nself = <tests.test_performance_monitoring_unit.TestPerformanceMonitoring object at 0x1178018d0>\naccessibility_module = <modules.accessibility.AccessibilityModule object at 0x117814710>\n\n    def test_performance_history_size_limit(self, accessibility_module):\n        \"\"\"Test that performance history respects size limits.\"\"\"\n        original_max_history = accessibility_module.max_performance_history\n    \n        try:\n            # Set a small history size for testing\n            accessibility_module.max_performance_history = 3\n    \n            # Add more metrics than the limit\n            for i in range(5):\n                metrics = PerformanceMetrics(f\"operation_{i}\", time.time())\n                metrics.finish()\n                accessibility_module.performance_metrics.append(metrics)\n    \n            # Should not exceed the maximum\n>           assert len(accessibility_module.performance_metrics) <= accessibility_module.max_performance_history\nE           AssertionError: assert 5 <= 3\nE            +  where 5 = len([PerformanceMetrics(operation_name='operation_0', start_time=1757096763.7034502, end_time=1757096763.7034519, duration_ms=0.0016689300537109375, timeout_ms=None, timed_out=False, success=True, error_message=None, metadata=None), PerformanceMetrics(operation_name='operation_1', start_time=1757096763.7034528, end_time=1757096763.7034528, duration_ms=0.0, timeout_ms=None, timed_out=False, success=True, error_message=None, metadata=None), PerformanceMetrics(operation_name='operation_2', start_time=1757096763.7034528, end_time=1757096763.703454, duration_ms=0.0011920928955078125, timeout_ms=None, timed_out=False, success=True, error_message=None, metadata=None), PerformanceMetrics(operation_name='operation_3', start_time=1757096763.703454, end_time=1757096763.703454, duration_ms=0.0, timeout_ms=None, timed_out=False, success=True, error_message=None, metadata=None), PerformanceMetrics(operation_name='operation_4', start_time=1757096763.703454, end_time=1757096763.703455, duration_ms=0.00095367431640625, timeout_ms=None, timed_out=False, success=True, error_message=None, metadata=None)])\nE            +    where [PerformanceMetrics(operation_name='operation_0', start_time=1757096763.7034502, end_time=1757096763.7034519, duration_ms=0.0016689300537109375, timeout_ms=None, timed_out=False, success=True, error_message=None, metadata=None), PerformanceMetrics(operation_name='operation_1', start_time=1757096763.7034528, end_time=1757096763.7034528, duration_ms=0.0, timeout_ms=None, timed_out=False, success=True, error_message=None, metadata=None), PerformanceMetrics(operation_name='operation_2', start_time=1757096763.7034528, end_time=1757096763.703454, duration_ms=0.0011920928955078125, timeout_ms=None, timed_out=False, success=True, error_message=None, metadata=None), PerformanceMetrics(operation_name='operation_3', start_time=1757096763.703454, end_time=1757096763.703454, duration_ms=0.0, timeout_ms=None, timed_out=False, success=True, error_message=None, metadata=None), PerformanceMetrics(operation_name='operation_4', start_time=1757096763.703454, end_time=1757096763.703455, duration_ms=0.00095367431640625, timeout_ms=None, timed_out=False, success=True, error_message=None, metadata=None)] = <modules.accessibility.AccessibilityModule object at 0x117814710>.performance_metrics\nE            +  and   3 = <modules.accessibility.AccessibilityModule object at 0x117814710>.max_performance_history\n\ntests/test_performance_monitoring_unit.py:239: AssertionError\n------------------------------ Captured log setup ------------------------------\nWARNING  modules.accessibility:accessibility.py:540 Insufficient accessibility permissions for optimal performance\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility:accessibility.py:619 Failed to initialize debugging tools: 'dict' object has no attribute 'validate'\nWARNING  modules.accessibility:accessibility.py:503 Accessibility permissions not granted: Insufficient accessibility permissions: PARTIAL\nWARNING  modules.accessibility:accessibility.py:716 AccessibilityModule entering degraded mode: permission_denied\n________ TestCachingFunctionality.test_fuzzy_match_cache_functionality _________\n\nself = <tests.test_performance_monitoring_unit.TestCachingFunctionality object at 0x117801310>\naccessibility_module = <modules.accessibility.AccessibilityModule object at 0x1078c1a50>\n\n    def test_fuzzy_match_cache_functionality(self, accessibility_module):\n        \"\"\"Test fuzzy matching result caching.\"\"\"\n        assert hasattr(accessibility_module, 'fuzzy_match_cache')\n        assert isinstance(accessibility_module.fuzzy_match_cache, dict)\n    \n        # Test cache key generation\n>       cache_key = accessibility_module._generate_fuzzy_match_cache_key(\"element_text\", \"target_text\")\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: AccessibilityModule._generate_fuzzy_match_cache_key() missing 1 required positional argument: 'confidence_threshold'\n\ntests/test_performance_monitoring_unit.py:264: TypeError\n------------------------------ Captured log setup ------------------------------\nWARNING  modules.accessibility:accessibility.py:540 Insufficient accessibility permissions for optimal performance\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility:accessibility.py:619 Failed to initialize debugging tools: 'dict' object has no attribute 'validate'\nWARNING  modules.accessibility:accessibility.py:503 Accessibility permissions not granted: Insufficient accessibility permissions: PARTIAL\nWARNING  modules.accessibility:accessibility.py:716 AccessibilityModule entering degraded mode: permission_denied\n______________ TestCachingFunctionality.test_cache_ttl_expiration ______________\n\nself = <tests.test_performance_monitoring_unit.TestCachingFunctionality object at 0x117802090>\naccessibility_module = <modules.accessibility.AccessibilityModule object at 0x1078c1350>\n\n    def test_cache_ttl_expiration(self, accessibility_module):\n        \"\"\"Test cache TTL (time-to-live) expiration.\"\"\"\n        # Add item to cache with current timestamp\n        current_time = time.time()\n        cache_key = \"test_key\"\n        accessibility_module.fuzzy_match_cache[cache_key] = (True, 95.0, current_time)\n    \n        # Test that item is not expired immediately\n>       assert not accessibility_module._is_cache_expired(cache_key, current_time + 1)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       AttributeError: 'AccessibilityModule' object has no attribute '_is_cache_expired'\n\ntests/test_performance_monitoring_unit.py:287: AttributeError\n------------------------------ Captured log setup ------------------------------\nWARNING  modules.accessibility:accessibility.py:540 Insufficient accessibility permissions for optimal performance\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility:accessibility.py:619 Failed to initialize debugging tools: 'dict' object has no attribute 'validate'\nWARNING  modules.accessibility:accessibility.py:503 Accessibility permissions not granted: Insufficient accessibility permissions: PARTIAL\nWARNING  modules.accessibility:accessibility.py:716 AccessibilityModule entering degraded mode: permission_denied\n__________ TestCachingFunctionality.test_cache_size_limit_enforcement __________\n\nself = <tests.test_performance_monitoring_unit.TestCachingFunctionality object at 0x117802450>\naccessibility_module = <modules.accessibility.AccessibilityModule object at 0x117bedf90>\n\n    def test_cache_size_limit_enforcement(self, accessibility_module):\n        \"\"\"Test that cache size limits are enforced.\"\"\"\n        original_max_entries = accessibility_module.max_cache_entries\n    \n        try:\n            # Set a small cache size for testing\n            accessibility_module.max_cache_entries = 3\n    \n            # Add more entries than the limit\n            for i in range(5):\n                cache_key = f\"test_key_{i}\"\n                accessibility_module.fuzzy_match_cache[cache_key] = (True, 95.0, time.time())\n    \n            # Trigger cache cleanup\n>           accessibility_module._cleanup_expired_cache_entries()\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE           AttributeError: 'AccessibilityModule' object has no attribute '_cleanup_expired_cache_entries'\n\ntests/test_performance_monitoring_unit.py:307: AttributeError\n------------------------------ Captured log setup ------------------------------\nWARNING  modules.accessibility:accessibility.py:540 Insufficient accessibility permissions for optimal performance\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility:accessibility.py:619 Failed to initialize debugging tools: 'dict' object has no attribute 'validate'\nWARNING  modules.accessibility:accessibility.py:503 Accessibility permissions not granted: Insufficient accessibility permissions: PARTIAL\nWARNING  modules.accessibility:accessibility.py:716 AccessibilityModule entering degraded mode: permission_denied\n__________ TestCachingFunctionality.test_cache_cleanup_functionality ___________\n\nself = <tests.test_performance_monitoring_unit.TestCachingFunctionality object at 0x117802910>\naccessibility_module = <modules.accessibility.AccessibilityModule object at 0x117be6a90>\n\n    def test_cache_cleanup_functionality(self, accessibility_module):\n        \"\"\"Test cache cleanup removes expired entries.\"\"\"\n        # Add expired entry\n        old_time = time.time() - accessibility_module.cache_ttl_seconds - 1\n        accessibility_module.fuzzy_match_cache[\"expired_key\"] = (True, 95.0, old_time)\n    \n        # Add current entry\n        current_time = time.time()\n        accessibility_module.fuzzy_match_cache[\"current_key\"] = (True, 90.0, current_time)\n    \n        # Run cleanup\n>       accessibility_module._cleanup_expired_cache_entries()\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       AttributeError: 'AccessibilityModule' object has no attribute '_cleanup_expired_cache_entries'\n\ntests/test_performance_monitoring_unit.py:327: AttributeError\n------------------------------ Captured log setup ------------------------------\nWARNING  modules.accessibility:accessibility.py:540 Insufficient accessibility permissions for optimal performance\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility:accessibility.py:619 Failed to initialize debugging tools: 'dict' object has no attribute 'validate'\nWARNING  modules.accessibility:accessibility.py:503 Accessibility permissions not granted: Insufficient accessibility permissions: PARTIAL\nWARNING  modules.accessibility:accessibility.py:716 AccessibilityModule entering degraded mode: permission_denied\n_____ TestCachingFunctionality.test_cache_integration_with_fuzzy_matching ______\n\nself = <tests.test_performance_monitoring_unit.TestCachingFunctionality object at 0x117803590>\naccessibility_module = <modules.accessibility.AccessibilityModule object at 0x117baeed0>\n\n    def test_cache_integration_with_fuzzy_matching(self, accessibility_module):\n        \"\"\"Test that fuzzy matching integrates with caching.\"\"\"\n        # First call should miss cache and compute result\n>       with patch.object(accessibility_module, '_compute_fuzzy_match') as mock_compute:\n\ntests/test_performance_monitoring_unit.py:346: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/opt/anaconda3/envs/aura/lib/python3.11/unittest/mock.py:1446: in __enter__\n    original, local = self.get_original()\n                      ^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <unittest.mock._patch object at 0x117814ed0>\n\n    def get_original(self):\n        target = self.getter()\n        name = self.attribute\n    \n        original = DEFAULT\n        local = False\n    \n        try:\n            original = target.__dict__[name]\n        except (AttributeError, KeyError):\n            original = getattr(target, name, DEFAULT)\n        else:\n            local = True\n    \n        if name in _builtins and isinstance(target, ModuleType):\n            self.create = True\n    \n        if not self.create and original is DEFAULT:\n>           raise AttributeError(\n                \"%s does not have the attribute %r\" % (target, name)\n            )\nE           AttributeError: <modules.accessibility.AccessibilityModule object at 0x117baeed0> does not have the attribute '_compute_fuzzy_match'\n\n/opt/anaconda3/envs/aura/lib/python3.11/unittest/mock.py:1419: AttributeError\n------------------------------ Captured log setup ------------------------------\nWARNING  modules.accessibility:accessibility.py:540 Insufficient accessibility permissions for optimal performance\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility:accessibility.py:619 Failed to initialize debugging tools: 'dict' object has no attribute 'validate'\nWARNING  modules.accessibility:accessibility.py:503 Accessibility permissions not granted: Insufficient accessibility permissions: PARTIAL\nWARNING  modules.accessibility:accessibility.py:716 AccessibilityModule entering degraded mode: permission_denied\n_________ TestTimeoutHandling.test_attribute_checking_timeout_handling _________\n\nself = <tests.test_performance_monitoring_unit.TestTimeoutHandling object at 0x117808b10>\naccessibility_module = <modules.accessibility.AccessibilityModule object at 0x117bd4890>\n\n    def test_attribute_checking_timeout_handling(self, accessibility_module):\n        \"\"\"Test timeout handling in attribute checking operations.\"\"\"\n        mock_element = Mock()\n    \n        # Mock slow attribute access\n>       with patch.object(accessibility_module, '_get_element_attribute') as mock_get_attr:\n\ntests/test_performance_monitoring_unit.py:416: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/opt/anaconda3/envs/aura/lib/python3.11/unittest/mock.py:1446: in __enter__\n    original, local = self.get_original()\n                      ^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <unittest.mock._patch object at 0x117c54710>\n\n    def get_original(self):\n        target = self.getter()\n        name = self.attribute\n    \n        original = DEFAULT\n        local = False\n    \n        try:\n            original = target.__dict__[name]\n        except (AttributeError, KeyError):\n            original = getattr(target, name, DEFAULT)\n        else:\n            local = True\n    \n        if name in _builtins and isinstance(target, ModuleType):\n            self.create = True\n    \n        if not self.create and original is DEFAULT:\n>           raise AttributeError(\n                \"%s does not have the attribute %r\" % (target, name)\n            )\nE           AttributeError: <modules.accessibility.AccessibilityModule object at 0x117bd4890> does not have the attribute '_get_element_attribute'\n\n/opt/anaconda3/envs/aura/lib/python3.11/unittest/mock.py:1419: AttributeError\n------------------------------ Captured log setup ------------------------------\nWARNING  modules.accessibility:accessibility.py:540 Insufficient accessibility permissions for optimal performance\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility:accessibility.py:619 Failed to initialize debugging tools: 'dict' object has no attribute 'validate'\nWARNING  modules.accessibility:accessibility.py:503 Accessibility permissions not granted: Insufficient accessibility permissions: PARTIAL\nWARNING  modules.accessibility:accessibility.py:716 AccessibilityModule entering degraded mode: permission_denied\n_______________ TestTimeoutHandling.test_timeout_warning_logging _______________\n\nself = <tests.test_performance_monitoring_unit.TestTimeoutHandling object at 0x117809190>\naccessibility_module = <modules.accessibility.AccessibilityModule object at 0x117c56cd0>\n\n    def test_timeout_warning_logging(self, accessibility_module):\n        \"\"\"Test that timeout warnings are logged appropriately.\"\"\"\n        with patch.object(accessibility_module.performance_logger, 'warning') as mock_warning:\n            # Simulate an operation that exceeds timeout\n>           accessibility_module._log_timeout_warning(\"test_operation\", 1500, 1000)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE           AttributeError: 'AccessibilityModule' object has no attribute '_log_timeout_warning'\n\ntests/test_performance_monitoring_unit.py:439: AttributeError\n------------------------------ Captured log setup ------------------------------\nWARNING  modules.accessibility:accessibility.py:540 Insufficient accessibility permissions for optimal performance\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility:accessibility.py:619 Failed to initialize debugging tools: 'dict' object has no attribute 'validate'\nWARNING  modules.accessibility:accessibility.py:503 Accessibility permissions not granted: Insufficient accessibility permissions: PARTIAL\nWARNING  modules.accessibility:accessibility.py:716 AccessibilityModule entering degraded mode: permission_denied\n____________ TestTimeoutHandling.test_graceful_timeout_degradation _____________\n\nself = <tests.test_performance_monitoring_unit.TestTimeoutHandling object at 0x1178097d0>\naccessibility_module = <modules.accessibility.AccessibilityModule object at 0x117c55990>\n\n    def test_graceful_timeout_degradation(self, accessibility_module):\n        \"\"\"Test graceful degradation when operations timeout.\"\"\"\n        # Test that timeout doesn't crash the system\n>       with patch.object(accessibility_module, '_execute_with_timeout') as mock_execute:\n\ntests/test_performance_monitoring_unit.py:450: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/opt/anaconda3/envs/aura/lib/python3.11/unittest/mock.py:1446: in __enter__\n    original, local = self.get_original()\n                      ^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <unittest.mock._patch object at 0x1177fb090>\n\n    def get_original(self):\n        target = self.getter()\n        name = self.attribute\n    \n        original = DEFAULT\n        local = False\n    \n        try:\n            original = target.__dict__[name]\n        except (AttributeError, KeyError):\n            original = getattr(target, name, DEFAULT)\n        else:\n            local = True\n    \n        if name in _builtins and isinstance(target, ModuleType):\n            self.create = True\n    \n        if not self.create and original is DEFAULT:\n>           raise AttributeError(\n                \"%s does not have the attribute %r\" % (target, name)\n            )\nE           AttributeError: <modules.accessibility.AccessibilityModule object at 0x117c55990> does not have the attribute '_execute_with_timeout'\n\n/opt/anaconda3/envs/aura/lib/python3.11/unittest/mock.py:1419: AttributeError\n------------------------------ Captured log setup ------------------------------\nWARNING  modules.accessibility:accessibility.py:540 Insufficient accessibility permissions for optimal performance\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility_debugger:accessibility_debugger.py:235 Accessibility functions not available - debugging capabilities limited\nWARNING  modules.accessibility:accessibility.py:619 Failed to initialize debugging tools: 'dict' object has no attribute 'validate'\nWARNING  modules.accessibility:accessibility.py:503 Accessibility permissions not granted: Insufficient accessibility permissions: PARTIAL\nWARNING  modules.accessibility:accessibility.py:716 AccessibilityModule entering degraded mode: permission_denied\n=========================== short test summary info ============================\nFAILED tests/test_performance_monitoring_unit.py::TestPerformanceMonitoring::test_performance_timing_measurement\nFAILED tests/test_performance_monitoring_unit.py::TestPerformanceMonitoring::test_performance_warning_threshold\nFAILED tests/test_performance_monitoring_unit.py::TestPerformanceMonitoring::test_operation_metrics_tracking\nFAILED tests/test_performance_monitoring_unit.py::TestPerformanceMonitoring::test_performance_history_size_limit\nFAILED tests/test_performance_monitoring_unit.py::TestCachingFunctionality::test_fuzzy_match_cache_functionality\nFAILED tests/test_performance_monitoring_unit.py::TestCachingFunctionality::test_cache_ttl_expiration\nFAILED tests/test_performance_monitoring_unit.py::TestCachingFunctionality::test_cache_size_limit_enforcement\nFAILED tests/test_performance_monitoring_unit.py::TestCachingFunctionality::test_cache_cleanup_functionality\nFAILED tests/test_performance_monitoring_unit.py::TestCachingFunctionality::test_cache_integration_with_fuzzy_matching\nFAILED tests/test_performance_monitoring_unit.py::TestTimeoutHandling::test_attribute_checking_timeout_handling\nFAILED tests/test_performance_monitoring_unit.py::TestTimeoutHandling::test_timeout_warning_logging\nFAILED tests/test_performance_monitoring_unit.py::TestTimeoutHandling::test_graceful_timeout_degradation\n======================== 12 failed, 10 passed in 0.45s =========================\n",
            "stderr": ""
          }
        },
        {
          "test_name": "-v",
          "status": "FAILED",
          "execution_time": 0.2551388740539551,
          "error_message": "",
          "details": {
            "stdout": "============================= test session starts ==============================\nplatform darwin -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /opt/anaconda3/envs/aura/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/prateeksrivastava/Documents/aura\nconfigfile: pytest.ini\nplugins: asyncio-1.1.0, anyio-4.10.0, mock-3.14.1\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 21 items\n\ntests/test_fast_path_performance_monitor.py::TestRollingAverageCalculator::test_initialization PASSED [  4%]\ntests/test_fast_path_performance_monitor.py::TestRollingAverageCalculator::test_add_value_and_average PASSED [  9%]\ntests/test_fast_path_performance_monitor.py::TestRollingAverageCalculator::test_window_size_limit PASSED [ 14%]\ntests/test_fast_path_performance_monitor.py::TestRollingAverageCalculator::test_trend_detection PASSED [ 19%]\ntests/test_fast_path_performance_monitor.py::TestRollingAverageCalculator::test_thread_safety PASSED [ 23%]\ntests/test_fast_path_performance_monitor.py::TestFastPathPerformanceMonitor::test_initialization PASSED [ 28%]\ntests/test_fast_path_performance_monitor.py::TestFastPathPerformanceMonitor::test_record_successful_execution PASSED [ 33%]\ntests/test_fast_path_performance_monitor.py::TestFastPathPerformanceMonitor::test_record_failed_execution PASSED [ 38%]\ntests/test_fast_path_performance_monitor.py::TestFastPathPerformanceMonitor::test_application_specific_tracking PASSED [ 42%]\ntests/test_fast_path_performance_monitor.py::TestFastPathPerformanceMonitor::test_success_rate_calculation PASSED [ 47%]\ntests/test_fast_path_performance_monitor.py::TestFastPathPerformanceMonitor::test_performance_statistics FAILED [ 52%]\ntests/test_fast_path_performance_monitor.py::TestFastPathPerformanceMonitor::test_critical_success_rate_alert PASSED [ 57%]\ntests/test_fast_path_performance_monitor.py::TestFastPathPerformanceMonitor::test_execution_time_alert PASSED [ 61%]\ntests/test_fast_path_performance_monitor.py::TestFastPathPerformanceMonitor::test_consecutive_failures_alert FAILED [ 66%]\ntests/test_fast_path_performance_monitor.py::TestFastPathPerformanceMonitor::test_alert_cooldown PASSED [ 71%]\ntests/test_fast_path_performance_monitor.py::TestFastPathPerformanceMonitor::test_should_suggest_diagnostics PASSED [ 76%]\ntests/test_fast_path_performance_monitor.py::TestFastPathPerformanceMonitor::test_performance_feedback_message PASSED [ 80%]\ntests/test_fast_path_performance_monitor.py::TestFastPathPerformanceMonitor::test_export_performance_data_json PASSED [ 85%]\ntests/test_fast_path_performance_monitor.py::TestFastPathPerformanceMonitor::test_export_performance_data_csv PASSED [ 90%]\ntests/test_fast_path_performance_monitor.py::TestFastPathPerformanceMonitor::test_performance_improvement_detection FAILED [ 95%]\ntests/test_fast_path_performance_monitor.py::TestFastPathPerformanceMonitor::test_thread_safety PASSED [100%]\n\n=================================== FAILURES ===================================\n__________ TestFastPathPerformanceMonitor.test_performance_statistics __________\n\nself = <tests.test_fast_path_performance_monitor.TestFastPathPerformanceMonitor object at 0x103134510>\n\n    def test_performance_statistics(self):\n        \"\"\"Test comprehensive performance statistics.\"\"\"\n        # Record some test metrics\n        for i in range(10):\n            success = i % 3 != 0  # 2/3 success rate\n            metric = FastPathMetric(\n                command=f\"test command {i}\",\n                app_name=\"TestApp\",\n                execution_time=1.0 + (i * 0.1),\n                element_detection_time=0.5 + (i * 0.05),\n                matching_time=0.2 + (i * 0.02),\n                success=success,\n                element_found=success,\n                fallback_triggered=not success\n            )\n            self.monitor.record_fast_path_execution(metric)\n    \n        stats = self.monitor.get_performance_statistics(time_window_minutes=60)\n    \n        # Check basic statistics\n        assert stats['total_executions'] == 10\n>       assert stats['successful_executions'] == 7  # 7 out of 10 succeeded\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       assert 6 == 7\n\ntests/test_fast_path_performance_monitor.py:238: AssertionError\n------------------------------ Captured log call -------------------------------\nWARNING  modules.fast_path_performance_monitor:fast_path_performance_monitor.py:378 Performance Alert [MEDIUM]: Fast path success rate below optimal: 60.0%\n________ TestFastPathPerformanceMonitor.test_consecutive_failures_alert ________\n\nself = <tests.test_fast_path_performance_monitor.TestFastPathPerformanceMonitor object at 0x103135810>\n\n    def test_consecutive_failures_alert(self):\n        \"\"\"Test consecutive failures alert for specific app.\"\"\"\n        # Record consecutive failures for same app\n        for i in range(6):  # Above threshold of 5\n            metric = FastPathMetric(\n                command=\"test\",\n                app_name=\"ProblematicApp\",\n                execution_time=1.0,\n                success=False,\n                fallback_triggered=True\n            )\n            self.monitor.record_fast_path_execution(metric)\n    \n        # Should have triggered consecutive failures alert\n>       assert self.alert_callback.called\nE       AssertionError: assert False\nE        +  where False = <Mock id='4346223056'>.called\nE        +    where <Mock id='4346223056'> = <tests.test_fast_path_performance_monitor.TestFastPathPerformanceMonitor object at 0x103135810>.alert_callback\n\ntests/test_fast_path_performance_monitor.py:317: AssertionError\n____ TestFastPathPerformanceMonitor.test_performance_improvement_detection _____\n\nself = <tests.test_fast_path_performance_monitor.TestFastPathPerformanceMonitor object at 0x1031381d0>\n\n    def test_performance_improvement_detection(self):\n        \"\"\"Test detection of performance improvements.\"\"\"\n        # Start with poor performance\n        for i in range(10):\n            metric = FastPathMetric(\n                command=\"test\",\n                execution_time=3.0,\n                success=False,\n                fallback_triggered=True\n            )\n            self.monitor.record_fast_path_execution(metric)\n    \n        # Then improve performance significantly\n        for i in range(20):\n            metric = FastPathMetric(\n                command=\"test\",\n                execution_time=0.5,\n                success=True\n            )\n            self.monitor.record_fast_path_execution(metric)\n    \n        # Should detect improvement trend\n        success_trend = self.monitor.success_rate_calculator.get_trend()\n        time_trend = self.monitor.execution_time_calculator.get_trend()\n    \n        assert success_trend == \"improving\"\n>       assert time_trend == \"improving\"\nE       AssertionError: assert 'degrading' == 'improving'\nE         \nE         - improving\nE         + degrading\n\ntests/test_fast_path_performance_monitor.py:481: AssertionError\n------------------------------ Captured log call -------------------------------\nCRITICAL modules.fast_path_performance_monitor:fast_path_performance_monitor.py:378 Performance Alert [CRITICAL]: Fast path success rate critically low: 0.0%\nWARNING  modules.fast_path_performance_monitor:fast_path_performance_monitor.py:378 Performance Alert [MEDIUM]: Fast path execution time slower than optimal: 3.00s\nWARNING  modules.fast_path_performance_monitor:fast_path_performance_monitor.py:378 Performance Alert [MEDIUM]: Fast path success rate below optimal: 50.0%\nWARNING  modules.fast_path_performance_monitor:fast_path_performance_monitor.py:378 Performance Alert [MEDIUM]: Fast path execution time is increasing over time\n=========================== short test summary info ============================\nFAILED tests/test_fast_path_performance_monitor.py::TestFastPathPerformanceMonitor::test_performance_statistics\nFAILED tests/test_fast_path_performance_monitor.py::TestFastPathPerformanceMonitor::test_consecutive_failures_alert\nFAILED tests/test_fast_path_performance_monitor.py::TestFastPathPerformanceMonitor::test_performance_improvement_detection\n========================= 3 failed, 18 passed in 0.07s =========================\n",
            "stderr": ""
          }
        },
        {
          "test_name": "-v",
          "status": "PASSED",
          "execution_time": 0.46727895736694336,
          "error_message": null,
          "details": {
            "stdout": "============================= test session starts ==============================\nplatform darwin -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /opt/anaconda3/envs/aura/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/prateeksrivastava/Documents/aura\nconfigfile: pytest.ini\nplugins: asyncio-1.1.0, anyio-4.10.0, mock-3.14.1\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 25 items\n\ntests/test_diagnostic_tools.py::TestDiagnosticIssue::test_diagnostic_issue_creation PASSED [  4%]\ntests/test_diagnostic_tools.py::TestDiagnosticIssue::test_diagnostic_issue_to_dict PASSED [  8%]\ntests/test_diagnostic_tools.py::TestPerformanceBenchmark::test_performance_benchmark_creation PASSED [ 12%]\ntests/test_diagnostic_tools.py::TestPerformanceBenchmark::test_performance_benchmark_to_dict PASSED [ 16%]\ntests/test_diagnostic_tools.py::TestAccessibilityHealthChecker::test_analyze_system_issues_critical_permissions PASSED [ 20%]\ntests/test_diagnostic_tools.py::TestAccessibilityHealthChecker::test_analyze_system_issues_performance_problems PASSED [ 24%]\ntests/test_diagnostic_tools.py::TestAccessibilityHealthChecker::test_calculate_health_score_critical_issues PASSED [ 28%]\ntests/test_diagnostic_tools.py::TestAccessibilityHealthChecker::test_calculate_health_score_perfect_system PASSED [ 32%]\ntests/test_diagnostic_tools.py::TestAccessibilityHealthChecker::test_check_accessibility_api_health PASSED [ 36%]\ntests/test_diagnostic_tools.py::TestAccessibilityHealthChecker::test_check_accessibility_permissions PASSED [ 40%]\ntests/test_diagnostic_tools.py::TestAccessibilityHealthChecker::test_gather_system_information PASSED [ 44%]\ntests/test_diagnostic_tools.py::TestAccessibilityHealthChecker::test_gather_system_information_with_psutil PASSED [ 48%]\ntests/test_diagnostic_tools.py::TestAccessibilityHealthChecker::test_generate_recommendations_critical_issues PASSED [ 52%]\ntests/test_diagnostic_tools.py::TestAccessibilityHealthChecker::test_generate_recommendations_healthy_system PASSED [ 56%]\ntests/test_diagnostic_tools.py::TestAccessibilityHealthChecker::test_get_available_test_applications PASSED [ 60%]\ntests/test_diagnostic_tools.py::TestAccessibilityHealthChecker::test_health_checker_initialization PASSED [ 64%]\ntests/test_diagnostic_tools.py::TestDiagnosticReportGenerator::test_generate_issue_summary PASSED [ 68%]\ntests/test_diagnostic_tools.py::TestDiagnosticReportGenerator::test_generate_remediation_steps_permissions PASSED [ 72%]\ntests/test_diagnostic_tools.py::TestDiagnosticReportGenerator::test_generate_remediation_steps_system PASSED [ 76%]\ntests/test_diagnostic_tools.py::TestDiagnosticReportGenerator::test_prioritize_issues PASSED [ 80%]\ntests/test_diagnostic_tools.py::TestDiagnosticReportGenerator::test_report_generator_initialization PASSED [ 84%]\ntests/test_diagnostic_tools.py::TestDiagnosticReport::test_diagnostic_report_creation PASSED [ 88%]\ntests/test_diagnostic_tools.py::TestDiagnosticReport::test_export_report_json PASSED [ 92%]\ntests/test_diagnostic_tools.py::TestDiagnosticReport::test_export_report_text PASSED [ 96%]\ntests/test_diagnostic_tools.py::TestDiagnosticReport::test_generate_summary PASSED [100%]\n\n============================== 25 passed in 0.22s ==============================\n"
          }
        },
        {
          "test_name": "-v",
          "status": "PASSED",
          "execution_time": 5.568878889083862,
          "error_message": null,
          "details": {
            "stdout": "============================= test session starts ==============================\nplatform darwin -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /opt/anaconda3/envs/aura/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/prateeksrivastava/Documents/aura\nconfigfile: pytest.ini\nplugins: asyncio-1.1.0, anyio-4.10.0, mock-3.14.1\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 7 items\n\ntests/test_diagnostic_integration.py::TestDiagnosticWorkflowIntegration::test_complete_diagnostic_workflow_critical_issues PASSED [ 14%]\ntests/test_diagnostic_integration.py::TestDiagnosticWorkflowIntegration::test_complete_diagnostic_workflow_healthy_system PASSED [ 28%]\ntests/test_diagnostic_integration.py::TestDiagnosticWorkflowIntegration::test_enhanced_health_score_calculation PASSED [ 42%]\ntests/test_diagnostic_integration.py::TestDiagnosticWorkflowIntegration::test_intelligent_recommendations_generation PASSED [ 57%]\ntests/test_diagnostic_integration.py::TestDiagnosticWorkflowIntegration::test_issue_prioritization_workflow PASSED [ 71%]\ntests/test_diagnostic_integration.py::TestDiagnosticWorkflowIntegration::test_real_world_scenario_simulation PASSED [ 85%]\ntests/test_diagnostic_integration.py::TestDiagnosticWorkflowIntegration::test_report_export_formats PASSED [100%]\n\n============================== 7 passed in 5.29s ===============================\n"
          }
        }
      ],
      "success_rate": 44.44444444444444
    }
  },
  "debugging_effectiveness_analysis": {
    "permission_validation_effectiveness": false,
    "tree_inspection_effectiveness": false,
    "failure_analysis_effectiveness": false,
    "overall_debugging_effectiveness": 0.0,
    "critical_issues_identified": [
      "TestDebuggingFunctionalityValidation::test_permission_validation_comprehensive -v",
      "TestDebuggingFunctionalityValidation::test_accessibility_tree_inspection_comprehensive -v",
      "TestDebuggingFunctionalityValidation::test_element_detection_failure_analysis -v",
      "TestDebuggingFunctionalityValidation::test_comprehensive_diagnostics_execution -v"
    ]
  },
  "performance_impact_analysis": {
    "acceptable_overhead": false,
    "debug_level_scaling_working": false,
    "performance_impact_within_limits": false,
    "average_test_execution_time": 0.4452594518661499,
    "performance_issues_identified": [
      "TestDebuggingPerformanceImpact::test_debugging_overhead_measurement -v",
      "TestDebuggingPerformanceImpact::test_debug_level_performance_scaling -v"
    ]
  },
  "recommendations": [
    "Overall test success rate is below 80%. Review failed tests and address critical issues.",
    "Test suite 'debugging_functionality' has low success rate (0.0%). Review and fix failing tests.",
    "Failed tests in 'debugging_functionality': TestDebuggingFunctionalityValidation::test_permission_validation_comprehensive -v, TestDebuggingFunctionalityValidation::test_accessibility_tree_inspection_comprehensive -v, TestDebuggingFunctionalityValidation::test_element_detection_failure_analysis -v, TestDebuggingFunctionalityValidation::test_comprehensive_diagnostics_execution -v",
    "Test suite 'debugging_performance' has low success rate (0.0%). Review and fix failing tests.",
    "Failed tests in 'debugging_performance': TestDebuggingPerformanceImpact::test_debugging_overhead_measurement -v, TestDebuggingPerformanceImpact::test_debug_level_performance_scaling -v",
    "Test suite 'debugging_integration' has low success rate (0.0%). Review and fix failing tests.",
    "Failed tests in 'debugging_integration': TestDebuggingIntegrationWithExistingSystem::test_orchestrator_debugging_integration -v, TestDebuggingIntegrationWithExistingSystem::test_accessibility_module_debugging_integration -v",
    "Test suite 'real_world_scenarios' has low success rate (0.0%). Review and fix failing tests.",
    "Failed tests in 'real_world_scenarios': TestDebuggingRealWorldScenarios::test_browser_debugging_scenarios -v, TestDebuggingRealWorldScenarios::test_native_app_debugging_scenarios -v",
    "Test suite 'existing_system_integration' has low success rate (44.4%). Review and fix failing tests.",
    "Failed tests in 'existing_system_integration': -v, -v, -v, -v, -v",
    "Debugging performance tests indicate issues. Review performance impact and optimize debugging overhead.",
    "Debugging integration tests indicate issues. Review integration with existing system components."
  ]
}